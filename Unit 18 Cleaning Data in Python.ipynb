{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNb0iMqkYGd8VlRtE4kMdNS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namozhdehi/DataScience/blob/main/Unit%2018%20Cleaning%20Data%20in%20Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Unit 18**\n",
        "# **Cleaning Data in Python**"
      ],
      "metadata": {
        "id": "1LWaWKuMiXXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data type constraints**"
      ],
      "metadata": {
        "id": "7X_thulGjw0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_dict = {'duration': {0: '12 minutes',\n",
        "  1: '24 minutes',\n",
        "  2: '8 minutes',\n",
        "  3: '4 minutes',\n",
        "  4: '11 minutes'},\n",
        " 'station_A_id': {0: 81, 1: 3, 2: 67, 3: 16, 4: 22},\n",
        " 'station_A_name': {0: 'Berry St at 4th St',\n",
        "  1: 'Powell St BART Station (Market St at 4th St)',\n",
        "  2: 'San Francisco Caltrain Station 2  (Townsend St at 4th St)',\n",
        "  3: 'Steuart St at Market St',\n",
        "  4: 'Howard St at Beale St'},\n",
        " 'station_B_id': {0: 323, 1: 118, 2: 23, 3: 28, 4: 350},\n",
        " 'station_B_name': {0: 'Broadway at Kearny',\n",
        "  1: 'Eureka Valley Recreation Center',\n",
        "  2: 'The Embarcadero at Steuart St',\n",
        "  3: 'The Embarcadero at Bryant St',\n",
        "  4: '8th St at Brannan St'},\n",
        " 'bike_id': {0: 5480, 1: 5193, 2: 3652, 3: 1883, 4: 4626},\n",
        " 'user_type': {0: 2, 1: 2, 2: 3, 3: 1, 4: 2},\n",
        " 'user_birth_year': {0: 1959, 1: 1965, 2: 1993, 3: 1979, 4: 1994},\n",
        " 'user_gender': {0: 'Male', 1: 'Male', 2: 'Male', 3: 'Male', 4: 'Male'}}\n",
        "\n",
        "ride_sharing = pd.DataFrame(df_dict)"
      ],
      "metadata": {
        "id": "z7vabuWRmuZY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeSoIv43iUN7",
        "outputId": "3d76f4fa-5d42-4fe5-ee71-56cbc785dfe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 5 entries, 0 to 4\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   duration         5 non-null      object\n",
            " 1   station_A_id     5 non-null      int64 \n",
            " 2   station_A_name   5 non-null      object\n",
            " 3   station_B_id     5 non-null      int64 \n",
            " 4   station_B_name   5 non-null      object\n",
            " 5   bike_id          5 non-null      int64 \n",
            " 6   user_type        5 non-null      int64 \n",
            " 7   user_birth_year  5 non-null      int64 \n",
            " 8   user_gender      5 non-null      object\n",
            "dtypes: int64(5), object(4)\n",
            "memory usage: 400.0+ bytes\n",
            "None\n",
            "count    5.000000\n",
            "mean     2.000000\n",
            "std      0.707107\n",
            "min      1.000000\n",
            "25%      2.000000\n",
            "50%      2.000000\n",
            "75%      2.000000\n",
            "max      3.000000\n",
            "Name: user_type, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Print the information of ride_sharing\n",
        "print(ride_sharing.info())\n",
        "\n",
        "# Print summary statistics of user_type column\n",
        "print(ride_sharing['user_type'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the information of ride_sharing\n",
        "print(ride_sharing.info())\n",
        "\n",
        "# Print summary statistics of user_type column\n",
        "print(ride_sharing['user_type'].describe())\n",
        "\n",
        "# Convert user_type from integer to category\n",
        "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
        "\n",
        "# Write an assert statement confirming the change\n",
        "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
        "\n",
        "# Print new summary statistics\n",
        "print(ride_sharing['user_type_cat'].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbPqlTRAnFDg",
        "outputId": "c03bd566-7f67-4998-f7a6-311903e56592"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 5 entries, 0 to 4\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   duration         5 non-null      object\n",
            " 1   station_A_id     5 non-null      int64 \n",
            " 2   station_A_name   5 non-null      object\n",
            " 3   station_B_id     5 non-null      int64 \n",
            " 4   station_B_name   5 non-null      object\n",
            " 5   bike_id          5 non-null      int64 \n",
            " 6   user_type        5 non-null      int64 \n",
            " 7   user_birth_year  5 non-null      int64 \n",
            " 8   user_gender      5 non-null      object\n",
            "dtypes: int64(5), object(4)\n",
            "memory usage: 400.0+ bytes\n",
            "None\n",
            "count    5.000000\n",
            "mean     2.000000\n",
            "std      0.707107\n",
            "min      1.000000\n",
            "25%      2.000000\n",
            "50%      2.000000\n",
            "75%      2.000000\n",
            "max      3.000000\n",
            "Name: user_type, dtype: float64\n",
            "count     5\n",
            "unique    3\n",
            "top       2\n",
            "freq      3\n",
            "Name: user_type_cat, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip duration of minutes\n",
        "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')\n",
        "\n",
        "# Convert duration to integer\n",
        "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
        "\n",
        "# Write an assert statement making sure of conversion\n",
        "assert ride_sharing['duration_time'].dtype == 'int'\n",
        "\n",
        "# Print formed columns and calculate average ride duration\n",
        "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
        "print(ride_sharing['duration_time'].mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQVUL8NZowTO",
        "outputId": "a23fbba2-804b-4c6b-b6ef-2078b7f5aef5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     duration duration_trim  duration_time\n",
            "0  12 minutes           12              12\n",
            "1  24 minutes           24              24\n",
            "2   8 minutes            8               8\n",
            "3   4 minutes            4               4\n",
            "4  11 minutes           11              11\n",
            "11.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data range constraints**"
      ],
      "metadata": {
        "id": "8srHpGwCqqsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = {'duration': {0: '12 minutes',\n",
        "  1: '24 minutes',\n",
        "  2: '8 minutes',\n",
        "  3: '4 minutes',\n",
        "  4: '11 minutes'},\n",
        " 'station_A_id': {0: 81, 1: 3, 2: 67, 3: 16, 4: 22},\n",
        " 'station_A_name': {0: 'Berry St at 4th St',\n",
        "  1: 'Powell St BART Station (Market St at 4th St)',\n",
        "  2: 'San Francisco Caltrain Station 2  (Townsend St at 4th St)',\n",
        "  3: 'Steuart St at Market St',\n",
        "  4: 'Howard St at Beale St'},\n",
        " 'station_B_id': {0: 323, 1: 118, 2: 23, 3: 28, 4: 350},\n",
        " 'station_B_name': {0: 'Broadway at Kearny',\n",
        "  1: 'Eureka Valley Recreation Center',\n",
        "  2: 'The Embarcadero at Steuart St',\n",
        "  3: 'The Embarcadero at Bryant St',\n",
        "  4: '8th St at Brannan St'},\n",
        " 'bike_id': {0: 5480, 1: 5193, 2: 3652, 3: 1883, 4: 4626},\n",
        " 'user_type': {0: 'Subscriber',\n",
        "  1: 'Subscriber',\n",
        "  2: 'Subscriber',\n",
        "  3: 'Subscriber',\n",
        "  4: 'Subscriber'},\n",
        " 'user_birth_year': {0: 1959, 1: 1965, 2: 1993, 3: 1979, 4: 1994},\n",
        " 'user_gender': {0: 'Male', 1: 'Male', 2: 'Male', 3: 'Male', 4: 'Male'},\n",
        " 'tire_sizes': {0: 27, 1: 26, 2: 26, 3: 27, 4: 27}}\n",
        "ride_sharing = pd.DataFrame(df_dict)"
      ],
      "metadata": {
        "id": "NpSKo7bH1ChR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tire_sizes to integer\n",
        "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
        "\n",
        "# Set all values above 27 to 27\n",
        "ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
        "\n",
        "# Reconvert tire_sizes back to categorical\n",
        "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
        "\n",
        "# Print tire size description\n",
        "print(ride_sharing['tire_sizes'].describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-7iv2o0p2jy",
        "outputId": "2f2a8313-42b4-4dc0-d50b-b85d32e792e9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count      5\n",
            "unique     2\n",
            "top       27\n",
            "freq       3\n",
            "Name: tire_sizes, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "df_dict = {'duration': {0: '12 minutes',\n",
        "  1: '24 minutes',\n",
        "  2: '8 minutes',\n",
        "  3: '4 minutes',\n",
        "  4: '11 minutes'},\n",
        " 'station_A_id': {0: 81, 1: 3, 2: 67, 3: 16, 4: 22},\n",
        " 'station_A_name': {0: 'Berry St at 4th St',\n",
        "  1: 'Powell St BART Station (Market St at 4th St)',\n",
        "  2: 'San Francisco Caltrain Station 2  (Townsend St at 4th St)',\n",
        "  3: 'Steuart St at Market St',\n",
        "  4: 'Howard St at Beale St'},\n",
        " 'station_B_id': {0: 323, 1: 118, 2: 23, 3: 28, 4: 350},\n",
        " 'station_B_name': {0: 'Broadway at Kearny',\n",
        "  1: 'Eureka Valley Recreation Center',\n",
        "  2: 'The Embarcadero at Steuart St',\n",
        "  3: 'The Embarcadero at Bryant St',\n",
        "  4: '8th St at Brannan St'},\n",
        " 'bike_id': {0: 5480, 1: 5193, 2: 3652, 3: 1883, 4: 4626},\n",
        " 'user_type': {0: 'Subscriber',\n",
        "  1: 'Subscriber',\n",
        "  2: 'Subscriber',\n",
        "  3: 'Subscriber',\n",
        "  4: 'Subscriber'},\n",
        " 'user_birth_year': {0: 1959, 1: 1965, 2: 1993, 3: 1979, 4: 1994},\n",
        " 'user_gender': {0: 'Male', 1: 'Male', 2: 'Male', 3: 'Male', 4: 'Male'},\n",
        " 'tire_sizes': {0: 27.0, 1: 26.0, 2: 26.0, 3: 29.0, 4: 27.0},\n",
        " 'ride_date': {0: '2020-01-19',\n",
        "  1: '2018-10-24',\n",
        "  2: '2017-12-25',\n",
        "  3: datetime.date(2025, 5, 15),\n",
        "  4: '2019-01-29'},\n",
        " 'ride_dt': {0: datetime.date(2020, 1, 19),\n",
        "  1: datetime.date(2018, 10, 24),\n",
        "  2: datetime.date(2017, 12, 25),\n",
        "  3: datetime.date(2024, 5, 15),\n",
        "  4: datetime.date(2019, 1, 29)}}\n",
        "ride_sharing = pd.DataFrame(df_dict)"
      ],
      "metadata": {
        "id": "H7ge6Wwk2wwn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "# Convert ride_date to date\n",
        "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date\n",
        "\n",
        "# Save today's date\n",
        "today = dt.date.today()\n",
        "\n",
        "# Set all in the future to today's date\n",
        "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
        "\n",
        "# Print maximum of ride_dt column\n",
        "print(ride_sharing['ride_dt'].max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-T1bR3U1gx8",
        "outputId": "bf96d6d8-f486-4306-a071-39f616370fb9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Uniqueness Constraints**"
      ],
      "metadata": {
        "id": "sIStmnmL9f9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = {'ride_id': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4},\n",
        " 'duration': {0: 11, 1: 8, 2: 11, 3: 7, 4: 11},\n",
        " 'station_A_id': {0: 16, 1: 3, 2: 15, 3: 21, 4: 81},\n",
        " 'station_A_name': {0: 'Steuart St at Market St',\n",
        "  1: 'Powell St BART Station (Market St at 4th St)',\n",
        "  2: 'San Francisco Ferry Building (Harry Bridges Plaza)',\n",
        "  3: 'Montgomery St BART Station (Market St at 2nd St)',\n",
        "  4: 'Berry St at 4th St'},\n",
        " 'station_B_id': {0: 93, 1: 93, 2: 67, 3: 50, 4: 21},\n",
        " 'station_B_name': {0: '4th St at Mission Bay Blvd S',\n",
        "  1: '4th St at Mission Bay Blvd S',\n",
        "  2: 'San Francisco Caltrain Station 2  (Townsend St at 4th St)',\n",
        "  3: '2nd St at Townsend St',\n",
        "  4: 'Montgomery St BART Station (Market St at 2nd St)'},\n",
        " 'bike_id': {0: 5504, 1: 2915, 2: 5340, 3: 746, 4: 5477},\n",
        " 'user_type': {0: 'Subscriber',\n",
        "  1: 'Subscriber',\n",
        "  2: 'Customer',\n",
        "  3: 'Subscriber',\n",
        "  4: 'Subscriber'},\n",
        " 'user_birth_year': {0: 1988, 1: 1988, 2: 1988, 3: 1969, 4: 1986},\n",
        " 'user_gender': {0: 'Male', 1: 'Male', 2: 'Male', 3: 'Male', 4: 'Male'},\n",
        " 'tire_sizes': {0: 27, 1: 27, 2: 26, 3: 27, 4: 26},\n",
        " 'ride_date': {0: '2018-03-04',\n",
        "  1: '2017-03-27',\n",
        "  2: '2019-06-30',\n",
        "  3: '2018-11-16',\n",
        "  4: '2017-11-01'}}\n",
        "ride_sharing = pd.DataFrame(df_dict)"
      ],
      "metadata": {
        "id": "_-fIJ6-J9vwC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find duplicates\n",
        "duplicates = ride_sharing.duplicated(subset= ['ride_id'], keep = False)\n",
        "\n",
        "# Sort your duplicated rides\n",
        "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
        "\n",
        "# Print relevant columns of duplicated_rides\n",
        "print(duplicated_rides[['ride_id','duration','user_birth_year']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7HzpgYC2ESV",
        "outputId": "eac0003a-2cea-4c1e-d353-44d35d5983c5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [ride_id, user_birth_year, user_birth_year]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop complete duplicates from ride_sharing\n",
        "ride_dup = ride_sharing.drop_duplicates()\n",
        "\n",
        "# Create statistics dictionary for aggregation function\n",
        "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
        "\n",
        "# Group by ride_id and compute new statistics\n",
        "ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
        "\n",
        "# Find duplicated values again\n",
        "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
        "duplicated_rides = ride_unique[duplicates == True]\n",
        "\n",
        "# Assert duplicates are processed\n",
        "assert duplicated_rides.shape[0] == 0"
      ],
      "metadata": {
        "id": "7251PQUu2ELx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Membership constraints"
      ],
      "metadata": {
        "id": "T378kcmDBD0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = {'cleanliness': {0: 'Clean',\n",
        "  1: 'Average',\n",
        "  2: 'Somewhat clean',\n",
        "  3: 'Somewhat dirty',\n",
        "  4: 'Dirty'},\n",
        " 'safety': {0: 'Neutral',\n",
        "  1: 'Very safe',\n",
        "  2: 'Somewhat safe',\n",
        "  3: 'Very unsafe',\n",
        "  4: 'Somewhat unsafe'},\n",
        " 'satisfaction': {0: 'Very satisfied',\n",
        "  1: 'Neutral',\n",
        "  2: 'Somewhat satisfied',\n",
        "  3: 'Somewhat unsatisfied',\n",
        "  4: 'Very unsatisfied'}}\n",
        "\n",
        "categories = pd.DataFrame(df_dict)"
      ],
      "metadata": {
        "id": "Ix5uyH0gAoAF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print categories DataFrame\n",
        "print(categories)\n",
        "\n",
        "# Print unique values of survey columns in airlines\n",
        "print('Cleanliness: ', categories['cleanliness'].unique(), \"\\n\")\n",
        "print('Safety: ', categories['safety'].unique(), \"\\n\")\n",
        "print('Satisfaction: ', categories['satisfaction'].unique(), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHV-uFaDKQsS",
        "outputId": "a7667a3f-dbc3-493a-8e2f-6ca3176b0875"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      cleanliness           safety          satisfaction\n",
            "0           Clean          Neutral        Very satisfied\n",
            "1         Average        Very safe               Neutral\n",
            "2  Somewhat clean    Somewhat safe    Somewhat satisfied\n",
            "3  Somewhat dirty      Very unsafe  Somewhat unsatisfied\n",
            "4           Dirty  Somewhat unsafe      Very unsatisfied\n",
            "Cleanliness:  ['Clean' 'Average' 'Somewhat clean' 'Somewhat dirty' 'Dirty'] \n",
            "\n",
            "Safety:  ['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'] \n",
            "\n",
            "Satisfaction:  ['Very satisfied' 'Neutral' 'Somewhat satisfied' 'Somewhat unsatisfied'\n",
            " 'Very unsatisfied'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = {'id': {0: 1351, 1: 373, 2: 2820, 3: 1157, 4: 2992},\n",
        " 'day': {0: 'Tuesday',\n",
        "  1: 'Friday',\n",
        "  2: 'Thursday',\n",
        "  3: 'Tuesday',\n",
        "  4: 'Wednesday'},\n",
        " 'airline': {0: 'UNITED INTL',\n",
        "  1: 'ALASKA',\n",
        "  2: 'DELTA',\n",
        "  3: 'SOUTHWEST',\n",
        "  4: 'AMERICAN'},\n",
        " 'destination': {0: 'KANSAI',\n",
        "  1: 'SAN JOSE DEL CABO',\n",
        "  2: 'LOS ANGELES',\n",
        "  3: 'LOS ANGELES',\n",
        "  4: 'MIAMI'},\n",
        " 'dest_region': {0: 'Asia',\n",
        "  1: 'Canada/Mexico',\n",
        "  2: 'West US',\n",
        "  3: 'West US',\n",
        "  4: 'East US'},\n",
        " 'dest_size': {0: 'Hub', 1: 'Small', 2: 'Hub', 3: 'Hub', 4: 'Hub'},\n",
        " 'boarding_area': {0: 'Gates 91-102',\n",
        "  1: 'Gates 50-59',\n",
        "  2: 'Gates 40-48',\n",
        "  3: 'Gates 20-39',\n",
        "  4: 'Gates 50-59'},\n",
        " 'dept_time': {0: '2018-12-31',\n",
        "  1: '2018-12-31',\n",
        "  2: '2018-12-31',\n",
        "  3: '2018-12-31',\n",
        "  4: '2018-12-31'},\n",
        " 'wait_min': {0: 115.0, 1: 135.0, 2: 70.0000000000001, 3: 190.0, 4: 559.0},\n",
        " 'cleanliness': {0: 'Clean',\n",
        "  1: 'Clean',\n",
        "  2: 'Average',\n",
        "  3: 'Clean',\n",
        "  4: 'Unacceptable'},\n",
        " 'safety': {0: 'Neutral',\n",
        "  1: 'Very safe',\n",
        "  2: 'Somewhat safe',\n",
        "  3: 'Very safe',\n",
        "  4: 'Very safe'},\n",
        " 'satisfaction': {0: 'Very satisfied',\n",
        "  1: 'Very satisfied',\n",
        "  2: 'Neutral',\n",
        "  3: 'Somewhat satisfied',\n",
        "  4: 'Somewhat satisfied'}}\n",
        "\n",
        "airlines = pd.DataFrame(df_dict)"
      ],
      "metadata": {
        "id": "IFggguFULgXj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print categories DataFrame\n",
        "print(categories)\n",
        "\n",
        "# Print unique values of survey columns in airlines\n",
        "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
        "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
        "print('Satisfaction: ', airlines['satisfaction'].unique(), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D7FB-H5Lmlb",
        "outputId": "3070a4f8-e654-48f6-c620-60ed02b13a26"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      cleanliness           safety          satisfaction\n",
            "0           Clean          Neutral        Very satisfied\n",
            "1         Average        Very safe               Neutral\n",
            "2  Somewhat clean    Somewhat safe    Somewhat satisfied\n",
            "3  Somewhat dirty      Very unsafe  Somewhat unsatisfied\n",
            "4           Dirty  Somewhat unsafe      Very unsatisfied\n",
            "Cleanliness:  ['Clean' 'Average' 'Unacceptable'] \n",
            "\n",
            "Safety:  ['Neutral' 'Very safe' 'Somewhat safe'] \n",
            "\n",
            "Satisfaction:  ['Very satisfied' 'Neutral' 'Somewhat satisfied'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the cleanliness category in airlines not in categories\n",
        "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
        "\n",
        "# Find rows with that category\n",
        "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
        "\n",
        "# Print rows with inconsistent category\n",
        "print(airlines[cat_clean_rows])\n",
        "\n",
        "# Print rows with consistent categories only\n",
        "print(airlines[~cat_clean_rows])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iYQLGsBLpPz",
        "outputId": "b1aaaefe-cc15-4b14-8916-d09c381c0cae"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id        day   airline destination dest_region dest_size boarding_area  \\\n",
            "4  2992  Wednesday  AMERICAN       MIAMI     East US       Hub   Gates 50-59   \n",
            "\n",
            "    dept_time  wait_min   cleanliness     safety        satisfaction  \n",
            "4  2018-12-31     559.0  Unacceptable  Very safe  Somewhat satisfied  \n",
            "     id       day      airline        destination    dest_region dest_size  \\\n",
            "0  1351   Tuesday  UNITED INTL             KANSAI           Asia       Hub   \n",
            "1   373    Friday       ALASKA  SAN JOSE DEL CABO  Canada/Mexico     Small   \n",
            "2  2820  Thursday        DELTA        LOS ANGELES        West US       Hub   \n",
            "3  1157   Tuesday    SOUTHWEST        LOS ANGELES        West US       Hub   \n",
            "\n",
            "  boarding_area   dept_time  wait_min cleanliness         safety  \\\n",
            "0  Gates 91-102  2018-12-31     115.0       Clean        Neutral   \n",
            "1   Gates 50-59  2018-12-31     135.0       Clean      Very safe   \n",
            "2   Gates 40-48  2018-12-31      70.0     Average  Somewhat safe   \n",
            "3   Gates 20-39  2018-12-31     190.0       Clean      Very safe   \n",
            "\n",
            "         satisfaction  \n",
            "0      Very satisfied  \n",
            "1      Very satisfied  \n",
            "2             Neutral  \n",
            "3  Somewhat satisfied  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Categorical variables**"
      ],
      "metadata": {
        "id": "v8mXugTUQgEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print unique values of both columns\n",
        "print(airlines['dest_region'].unique())\n",
        "print(airlines['dest_size'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejaBX80AQful",
        "outputId": "8b28f4b0-4db4-4ed4-afc7-65fa2c969617"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Asia' 'Canada/Mexico' 'West US' 'East US']\n",
            "['Hub' 'Small']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print unique values of both columns\n",
        "print(airlines['dest_region'].unique())\n",
        "print(airlines['dest_size'].unique())\n",
        "\n",
        "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
        "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
        "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t531f_6zNiGd",
        "outputId": "67dace1e-5bf2-4ea8-d282-5e9e2e216c23"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Asia' 'Canada/Mexico' 'West US' 'East US']\n",
            "['Hub' 'Small']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print unique values of both columns\n",
        "print(airlines['dest_region'].unique())\n",
        "print(airlines['dest_size'].unique())\n",
        "\n",
        "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
        "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
        "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
        "\n",
        "# Remove white spaces from `dest_size`\n",
        "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
        "\n",
        "# Verify changes have been effected\n",
        "print(airlines['dest_region'])\n",
        "print(airlines['dest_size'])"
      ],
      "metadata": {
        "id": "BYZoyG7TTNUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ranges for categories\n",
        "label_ranges = [0, 60, 180, np.inf]\n",
        "label_names = ['short', 'medium', 'long']\n",
        "\n",
        "# Create wait_type column\n",
        "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges,\n",
        "                                labels = label_names)\n",
        "\n",
        "# Create mappings and replace\n",
        "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday',\n",
        "            'Thursday': 'weekday', 'Friday': 'weekday',\n",
        "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
        "\n",
        "airlines['day_week'] = airlines['day'].replace(mappings)"
      ],
      "metadata": {
        "id": "bQ10e5fnTNPB"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = {'id': {0: 1351, 1: 373, 2: 2820, 3: 1157, 4: 2992},\n",
        " 'full_name': {0: 'Melodie Stuart',\n",
        "  1: 'Dominic Shannon',\n",
        "  2: 'Quintessa Tillman',\n",
        "  3: 'Dr. Christine Nicholson',\n",
        "  4: 'Regina Clements'},\n",
        " 'day': {0: 'Tuesday',\n",
        "  1: 'Friday',\n",
        "  2: 'Thursday',\n",
        "  3: 'Tuesday',\n",
        "  4: 'Wednesday'},\n",
        " 'airline': {0: 'UNITED INTL',\n",
        "  1: 'ALASKA',\n",
        "  2: 'DELTA',\n",
        "  3: 'SOUTHWEST',\n",
        "  4: 'AMERICAN'},\n",
        " 'destination': {0: 'KANSAI',\n",
        "  1: 'SAN JOSE DEL CABO',\n",
        "  2: 'LOS ANGELES',\n",
        "  3: 'LOS ANGELES',\n",
        "  4: 'MIAMI'},\n",
        " 'dest_region': {0: 'Asia',\n",
        "  1: 'Canada/Mexico',\n",
        "  2: 'West US',\n",
        "  3: 'West US',\n",
        "  4: 'East US'},\n",
        " 'dest_size': {0: 'Hub', 1: 'Small', 2: 'Hub', 3: 'Hub', 4: 'Hub'},\n",
        " 'boarding_area': {0: 'Gates 91-102',\n",
        "  1: 'Gates 50-59',\n",
        "  2: 'Gates 40-48',\n",
        "  3: 'Gates 20-39',\n",
        "  4: 'Gates 50-59'},\n",
        " 'dept_time': {0: '2018-12-31',\n",
        "  1: '2018-12-31',\n",
        "  2: '2018-12-31',\n",
        "  3: '2018-12-31',\n",
        "  4: '2018-12-31'},\n",
        " 'wait_min': {0: 115.0, 1: 135.0, 2: 70.0000000000001, 3: 190.0, 4: 559.0},\n",
        " 'cleanliness': {0: 'Clean',\n",
        "  1: 'Clean',\n",
        "  2: 'Average',\n",
        "  3: 'Clean',\n",
        "  4: 'Somewhat clean'},\n",
        " 'safety': {0: 'Neutral',\n",
        "  1: 'Very safe',\n",
        "  2: 'Somewhat safe',\n",
        "  3: 'Very safe',\n",
        "  4: 'Very safe'},\n",
        " 'satisfaction': {0: 'Very satisfied',\n",
        "  1: 'Very satisfied',\n",
        "  2: 'Neutral',\n",
        "  3: 'Somewhat satsified',\n",
        "  4: 'Somewhat satsified'}}\n",
        "airlines = pd.DataFrame(df_dict)"
      ],
      "metadata": {
        "id": "yTw4DfC5ZkoS"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"Dr.\" with empty string \"\"\n",
        "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n",
        "\n",
        "# Replace \"Mr.\" with empty string \"\"\n",
        "airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\",\"\")\n",
        "\n",
        "# Replace \"Miss\" with empty string \"\"\n",
        "airlines['full_name'] = airlines['full_name'].str.replace(\"Miss\", \"\")\n",
        "\n",
        "# Replace \"Ms.\" with empty string \"\"\n",
        "airlines['full_name'] = airlines['full_name'].str.replace(\"Ms.\", \"\")\n",
        "\n",
        "# Assert that full_name has no honorifics\n",
        "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False"
      ],
      "metadata": {
        "id": "kPzouTnyVBSu"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = {'id': {0: 1351, 1: 373, 2: 2820, 3: 1157, 4: 2992},\n",
        " 'day': {0: 'Tuesday',\n",
        "  1: 'Friday',\n",
        "  2: 'Thursday',\n",
        "  3: 'Tuesday',\n",
        "  4: 'Wednesday'},\n",
        " 'airline': {0: 'UNITED INTL',\n",
        "  1: 'ALASKA',\n",
        "  2: 'DELTA',\n",
        "  3: 'SOUTHWEST',\n",
        "  4: 'AMERICAN'},\n",
        " 'destination': {0: 'KANSAI',\n",
        "  1: 'SAN JOSE DEL CABO',\n",
        "  2: 'LOS ANGELES',\n",
        "  3: 'LOS ANGELES',\n",
        "  4: 'MIAMI'},\n",
        " 'dest_region': {0: 'Asia',\n",
        "  1: 'Canada/Mexico',\n",
        "  2: 'West US',\n",
        "  3: 'West US',\n",
        "  4: 'East US'},\n",
        " 'dest_size': {0: 'Hub', 1: 'Small', 2: 'Hub', 3: 'Hub', 4: 'Hub'},\n",
        " 'boarding_area': {0: 'Gates 91-102',\n",
        "  1: 'Gates 50-59',\n",
        "  2: 'Gates 40-48',\n",
        "  3: 'Gates 20-39',\n",
        "  4: 'Gates 50-59'},\n",
        " 'dept_time': {0: '2018-12-31',\n",
        "  1: '2018-12-31',\n",
        "  2: '2018-12-31',\n",
        "  3: '2018-12-31',\n",
        "  4: '2018-12-31'},\n",
        " 'wait_min': {0: 115.0, 1: 135.0, 2: 70.0000000000001, 3: 190.0, 4: 559.0},\n",
        " 'cleanliness': {0: 'Dirty', 1: 'Dirty', 2: 'Dirty', 3: 'Dirty', 4: 'Dirty'},\n",
        " 'safety': {0: 'Very unsafe',\n",
        "  1: 'Very unsafe',\n",
        "  2: 'Very unsafe',\n",
        "  3: 'Very unsafe',\n",
        "  4: 'Very unsafe'},\n",
        " 'satisfaction': {0: 'Very unsatisfied',\n",
        "  1: 'Very unsatisfied',\n",
        "  2: 'Very unsatisfied',\n",
        "  3: 'Very unsatisfied',\n",
        "  4: 'Very unsatisfied'},\n",
        " 'survey_response': {0: 'It was terrible',\n",
        "  1: \"I didn't like the flight\",\n",
        "  2: 'I hate this ',\n",
        "  3: 'Not a fan',\n",
        "  4: 'Bad'}}\n",
        "airlines = pd.DataFrame(df_dict)"
      ],
      "metadata": {
        "id": "7_gDjIYAbMAB"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store length of each row in survey_response column\n",
        "resp_length = airlines['survey_response'].str.len()\n",
        "\n",
        "# Find rows in airlines where resp_length > 40\n",
        "airlines_survey = airlines[resp_length > 40]\n",
        "\n",
        "# Print the minimum length of survey_response to debug\n",
        "print(\"Minimum length of survey_response in airlines_survey:\", airlines_survey['survey_response'].str.len().min())\n",
        "\n",
        "# Assert minimum survey_response length is > 40\n",
        "assert airlines_survey['survey_response'].str.len().min() > 40\n",
        "\n",
        "# Print new survey_response column\n",
        "print(airlines_survey['survey_response'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "iONwm4BAb520",
        "outputId": "7dcdf747-f6f3-421d-d6e9-34b03e0aeea2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum length of survey_response in airlines_survey: nan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-02ea1e37ac22>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Assert minimum survey_response length is > 40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mairlines_survey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'survey_response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Print new survey_response column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Uniformity**"
      ],
      "metadata": {
        "id": "R8LAGN_tcLcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = {'cust_id': {0: '8C35540A',\n",
        "  1: 'D5536652',\n",
        "  2: 'A631984D',\n",
        "  3: '93F2F951',\n",
        "  4: 'DE0A0882'},\n",
        " 'acct_amount': {0: 44244.71,\n",
        "  1: 86506.85,\n",
        "  2: 77799.33,\n",
        "  3: 93875.24,\n",
        "  4: 99998.35},\n",
        " 'acct_cur': {0: 'dollar', 1: 'dollar', 2: 'dollar', 3: 'euro', 4: 'euro'},\n",
        " 'inv_amount': {0: 35500.5,\n",
        "  1: 81921.86,\n",
        "  2: 46412.27,\n",
        "  3: 76563.35,\n",
        "  4: 18669.01},\n",
        " 'account_opened': {0: '03-05-18',\n",
        "  1: '21-01-18',\n",
        "  2: '26-01-18',\n",
        "  3: '21-08-17',\n",
        "  4: '05-06-17'},\n",
        " 'last_transaction': {0: '30-09-19',\n",
        "  1: '14-01-19',\n",
        "  2: '06-10-19',\n",
        "  3: '10-07-19',\n",
        "  4: '15-01-19'}}\n",
        "banking = pd.DataFrame(df_dict)"
      ],
      "metadata": {
        "id": "7gmH_hUqhi18"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find values of acct_cur that are equal to 'euro'\n",
        "acct_eu = banking['acct_cur'] == 'euro'\n",
        "\n",
        "# Convert acct_amount where it is in euro to dollars\n",
        "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\n",
        "\n",
        "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
        "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
        "\n",
        "# Assert that only dollar currency remains\n",
        "assert banking['acct_cur'].unique() == 'dollar'"
      ],
      "metadata": {
        "id": "qrSfChsea7Ri"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the header of account_opend\n",
        "print(banking['account_opened'].head())\n",
        "\n",
        "# Convert account_opened to datetime\n",
        "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
        "                                           # Infer datetime format\n",
        "                                           infer_datetime_format = True,\n",
        "                                           # Return missing value for error\n",
        "                                           errors = 'coerce')\n",
        "\n",
        "# Get year of account opened\n",
        "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
        "\n",
        "# Print acct_year\n",
        "print(banking['acct_year'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOZfu9OGi7ad",
        "outputId": "88266e50-8074-4834-fc95-45b9aafba10a"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0   2018-03-05\n",
            "1   2018-01-21\n",
            "2   2018-01-26\n",
            "3   2017-08-21\n",
            "4   2017-05-06\n",
            "Name: account_opened, dtype: datetime64[ns]\n",
            "0    2018\n",
            "1    2018\n",
            "2    2018\n",
            "3    2017\n",
            "4    2017\n",
            "Name: acct_year, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-67-ad1c82ffa5c5>:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cross field validation**"
      ],
      "metadata": {
        "id": "NJq8wkYajKVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from pandas import Timestamp\n",
        "\n",
        "df_dict = {'cust_id': {0: '870A9281',\n",
        "  1: '166B05B0',\n",
        "  2: 'BFC13E88',\n",
        "  3: 'F2158F66',\n",
        "  4: '7A73F334'},\n",
        " 'birth_date': {0: Timestamp('1962-06-09 00:00:00'),\n",
        "  1: Timestamp('1962-12-16 00:00:00'),\n",
        "  2: Timestamp('1990-09-12 00:00:00'),\n",
        "  3: Timestamp('1985-11-03 00:00:00'),\n",
        "  4: Timestamp('1990-05-17 00:00:00')},\n",
        " 'age': {0: 62, 1: 62, 2: 34, 3: 39, 4: 40},\n",
        " 'acct_amount': {0: 63523.31,\n",
        "  1: 38175.46,\n",
        "  2: 59863.77,\n",
        "  3: 84132.1,\n",
        "  4: 120512.0},\n",
        " 'inv_amount': {0: 51295, 1: 15050, 2: 24567, 3: 23712, 4: 93230},\n",
        " 'fund_A': {0: 30105.0, 1: 4995.0, 2: 10323.0, 3: 3908.0, 4: 12158.4},\n",
        " 'fund_B': {0: 4138.0, 1: 938.0, 2: 4590.0, 3: 492.0, 4: 51281.0},\n",
        " 'fund_C': {0: 1420.0, 1: 6696.0, 2: 8469.0, 3: 6482.0, 4: 13434.0},\n",
        " 'fund_D': {0: 15632.0, 1: 2421.0, 2: 1185.0, 3: 12830.0, 4: 18383.0},\n",
        " 'account_opened': {0: '02-09-18',\n",
        "  1: '28-02-19',\n",
        "  2: '25-04-18',\n",
        "  3: '07-11-17',\n",
        "  4: '14-05-18'},\n",
        " 'last_transaction': {0: '22-02-19',\n",
        "  1: '31-10-18',\n",
        "  2: '02-04-18',\n",
        "  3: '08-11-18',\n",
        "  4: '19-07-18'}}\n",
        "banking = pd.DataFrame(df_dict)\n"
      ],
      "metadata": {
        "id": "W0JF1pNik--W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store fund columns to sum against\n",
        "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
        "\n",
        "# Find rows where fund_columns row sum == inv_amount\n",
        "inv_equ = banking[fund_columns].sum(axis=1) == banking['inv_amount']\n",
        "\n",
        "# Store consistent and inconsistent data\n",
        "consistent_inv = banking[inv_equ]\n",
        "inconsistent_inv = banking[~inv_equ]\n",
        "\n",
        "# Store consistent and inconsistent data\n",
        "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6_M31thH4o6",
        "outputId": "29c03032-5ad4-4e46-c509-40eba5ba6781"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of inconsistent investments:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from pandas import Timestamp\n",
        "\n",
        "df_dict = {'cust_id': {0: '870A9281',\n",
        "  1: '166B05B0',\n",
        "  2: 'BFC13E88',\n",
        "  3: 'F2158F66',\n",
        "  4: '7A73F334'},\n",
        " 'birth_date': {0: Timestamp('1962-06-09 00:00:00'),\n",
        "  1: Timestamp('1962-12-16 00:00:00'),\n",
        "  2: Timestamp('1990-09-12 00:00:00'),\n",
        "  3: Timestamp('1985-11-03 00:00:00'),\n",
        "  4: Timestamp('1990-05-17 00:00:00')},\n",
        " 'age': {0: 62, 1: 62, 2: 34, 3: 39, 4: 40},\n",
        " 'acct_amount': {0: 63523.31,\n",
        "  1: 38175.46,\n",
        "  2: 59863.77,\n",
        "  3: 84132.1,\n",
        "  4: 120512.0},\n",
        " 'inv_amount': {0: 51295, 1: 15050, 2: 24567, 3: 23712, 4: 93230},\n",
        " 'fund_A': {0: 30105.0, 1: 4995.0, 2: 10323.0, 3: 3908.0, 4: 12158.4},\n",
        " 'fund_B': {0: 4138.0, 1: 938.0, 2: 4590.0, 3: 492.0, 4: 51281.0},\n",
        " 'fund_C': {0: 1420.0, 1: 6696.0, 2: 8469.0, 3: 6482.0, 4: 13434.0},\n",
        " 'fund_D': {0: 15632.0, 1: 2421.0, 2: 1185.0, 3: 12830.0, 4: 18383.0},\n",
        " 'account_opened': {0: '02-09-18',\n",
        "  1: '28-02-19',\n",
        "  2: '25-04-18',\n",
        "  3: '07-11-17',\n",
        "  4: '14-05-18'},\n",
        " 'last_transaction': {0: '22-02-19',\n",
        "  1: '31-10-18',\n",
        "  2: '02-04-18',\n",
        "  3: '08-11-18',\n",
        "  4: '19-07-18'}}\n",
        "banking = pd.DataFrame(df_dict)\n"
      ],
      "metadata": {
        "id": "GLwLZdTdKPu5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store today's date and find ages\n",
        "today = dt.date.today()\n",
        "ages_manual = today.year - banking['birth_date'].dt.year\n",
        "\n",
        "# Find rows where age column == ages_manual\n",
        "age_equ = banking['age'] == ages_manual\n",
        "\n",
        "# Store consistent and inconsistent data\n",
        "consistent_ages = banking[age_equ]\n",
        "inconsistent_ages = banking[~age_equ]\n",
        "\n",
        "# Store consistent and inconsistent data\n",
        "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3DQ6T5tIeeW",
        "outputId": "222004a8-4448-489c-e8d6-1285791878f0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of inconsistent ages:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Completeness**"
      ],
      "metadata": {
        "id": "rR8s7cbqL5VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import missingno as msno\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Print number of missing values in banking\n",
        "print(banking.isna().sum())\n",
        "\n",
        "# Visualize missingness matrix\n",
        "msno.matrix(banking)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdpNZfrlIgFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print number of missing values in banking\n",
        "print(banking.isna().sum())\n",
        "\n",
        "# Visualize missingness matrix\n",
        "msno.matrix(banking)\n",
        "plt.show()\n",
        "\n",
        "# Isolate missing and non missing values of inv_amount\n",
        "missing_investors = banking[banking['inv_amount'].isna()]\n",
        "investors = banking[~banking['inv_amount'].isna()]"
      ],
      "metadata": {
        "id": "HXUj0t_2If53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print number of missing values in banking\n",
        "print(banking.isna().sum())\n",
        "\n",
        "# Visualize missingness matrix\n",
        "msno.matrix(banking)\n",
        "plt.show()\n",
        "\n",
        "# Isolate missing and non missing values of inv_amount\n",
        "missing_investors = banking[banking['inv_amount'].isna()]\n",
        "investors = banking[~banking['inv_amount'].isna()]\n",
        "\n",
        "# Sort banking by age and visualize\n",
        "banking_sorted = banking.sort_values(by = 'age')\n",
        "msno.matrix(banking_sorted)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pd6tAXDmP6_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop missing values of cust_id\n",
        "banking_fullid = banking.dropna(subset = ['cust_id'])\n",
        "\n",
        "# Compute estimated acct_amount\n",
        "acct_imp = banking_fullid['inv_amount'] * 5\n",
        "\n",
        "# Impute missing acct_amount with corresponding acct_imp\n",
        "banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n",
        "\n",
        "# Print number of missing values\n",
        "print(banking_imputed.isna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEbepCq2QU26",
        "outputId": "cc3b921e-487b-48a3-9fbc-b33d2e974a63"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cust_id             0\n",
            "birth_date          0\n",
            "age                 0\n",
            "acct_amount         0\n",
            "inv_amount          0\n",
            "fund_A              0\n",
            "fund_B              0\n",
            "fund_C              0\n",
            "fund_D              0\n",
            "account_opened      0\n",
            "last_transaction    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparing strings**"
      ],
      "metadata": {
        "id": "nGgUHDKvRLYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install thefuzz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4zK15o-b8PK",
        "outputId": "9937569a-c5cc-4be3-d327-28adeacf2d8d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting thefuzz\n",
            "  Downloading thefuzz-0.22.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.0.0 (from thefuzz)\n",
            "  Downloading rapidfuzz-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, thefuzz\n",
            "Successfully installed rapidfuzz-3.9.0 thefuzz-0.22.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = {'rest_name': {0: 'arnie morton  s of chicago',\n",
        "  1: 'art  s delicatessen',\n",
        "  2: 'campanile',\n",
        "  3: 'fenix',\n",
        "  4: 'grill on the alley'},\n",
        " 'rest_addr': {0: ' 435 s. la cienega blv . ',\n",
        "  1: ' 12224 ventura blvd. ',\n",
        "  2: ' 624 s. la brea ave. ',\n",
        "  3: ' 8358 sunset blvd. west ',\n",
        "  4: ' 9560 dayton way '},\n",
        " 'city': {0: 'los angeles',\n",
        "  1: 'studio city',\n",
        "  2: 'los angeles',\n",
        "  3: 'hollywood',\n",
        "  4: 'los angeles'},\n",
        " 'phone': {0: 3102461501,\n",
        "  1: 8187621221,\n",
        "  2: 2139381447,\n",
        "  3: 2138486677,\n",
        "  4: 3102760615},\n",
        " 'cuisine_type': {0: 'america',\n",
        "  1: 'merican',\n",
        "  2: 'amurican',\n",
        "  3: 'americen',\n",
        "  4: 'americann'}}\n",
        "restaurants = pd.DataFrame(df_dict)"
      ],
      "metadata": {
        "id": "fIesm4O9ccTl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import process from thefuzz\n",
        "from thefuzz import fuzz\n",
        "from thefuzz import process\n",
        "\n",
        "# Store the unique values of cuisine_type in unique_types\n",
        "unique_types = restaurants['cuisine_type'].unique()\n",
        "\n",
        "# Calculate similarity of 'asian' to all values of unique_types\n",
        "print(process.extract('asian', unique_types, limit = len(unique_types)))\n",
        "\n",
        "# Calculate similarity of 'american' to all values of unique_types\n",
        "print(process.extract('american', unique_types, limit = len(unique_types)))\n",
        "\n",
        "# Calculate similarity of 'italian' to all values of unique_types\n",
        "print(process.extract('italian', unique_types, limit = len(unique_types)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwAt2vJNRNmv",
        "outputId": "25074c2d-b7b0-430e-b3c5-867e5485b6e7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('amurican', 62), ('americann', 57), ('america', 50), ('merican', 50), ('americen', 46)]\n",
            "[('americann', 94), ('america', 93), ('merican', 93), ('amurican', 88), ('americen', 88)]\n",
            "[('amurican', 53), ('americann', 50), ('america', 43), ('merican', 43), ('americen', 40)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the unique values of the cuisine_type column\n",
        "print(restaurants['cuisine_type'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVfYLVq-RNwH",
        "outputId": "51272eb6-62b6-42a6-e23a-b815efc85444"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['america' 'merican' 'amurican' 'americen' 'americann']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
        "matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants['cuisine_type']))\n",
        "\n",
        "# Inspect the first 5 matches\n",
        "print(matches[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPb43DqEdKl-",
        "outputId": "8076a7d0-4fdc-4ea9-d465-802cbdbfdb36"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('amurican', 53, 2), ('americann', 50, 4), ('america', 43, 0), ('merican', 43, 1), ('americen', 40, 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
        "matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
        "\n",
        "# Iterate through the list of matches to italian\n",
        "for match in matches:\n",
        "  # Check whether the similarity score is greater than or equal to 80\n",
        "  if match[1] >= 80:\n",
        "    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
        "    restaurants.loc[restaurants['cuisine_type'] == match[0]] = 'italian'"
      ],
      "metadata": {
        "id": "nLh-QLRTeaJ_"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through categories\n",
        "for cuisine in categories:\n",
        "  # Create a list of matches, comparing cuisine with the cuisine_type column\n",
        "  matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
        "\n",
        "  # Iterate through the list of matches\n",
        "  for match in matches:\n",
        "     # Check whether the similarity score is greater than or equal to 80\n",
        "    if match[1] >= 80:\n",
        "      # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
        "      restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
        "\n",
        "# Inspect the final result\n",
        "print(restaurants['cuisine_type'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "bvPNv4AKiFcm",
        "outputId": "12cb1780-2a15-4e0f-c3ba-5144f8f4ab2a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'categories' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-bd3af2b3caec>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Iterate through categories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcuisine\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;31m# Create a list of matches, comparing cuisine with the cuisine_type column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuisine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestaurants\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cuisine_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestaurants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuisine_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'categories' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generating pairs**"
      ],
      "metadata": {
        "id": "EAONDv8hjiy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install recordlinkage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt6MoHzcj7J3",
        "outputId": "dcd92ce7-9c75-42be-fddd-e69cb62089ec"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: recordlinkage in /usr/local/lib/python3.10/dist-packages (0.16)\n",
            "Requirement already satisfied: jellyfish>=1 in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (1.25.2)\n",
            "Requirement already satisfied: pandas<3,>=1 in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (2.0.3)\n",
            "Requirement already satisfied: scipy>=1 in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1 in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (1.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1->recordlinkage) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1->recordlinkage) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1->recordlinkage) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1->recordlinkage) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1->recordlinkage) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = {'rest_name': {0: 'kokomo', 1: 'feenix', 2: 'parkway', 3: 'r-23', 4: 'gumbo'},\n",
        " 'rest_addr': {0: '6333 w. third st.',\n",
        "  1: ' 8358 sunset blvd. west ',\n",
        "  2: '510 s. arroyo pkwy .',\n",
        "  3: '923 e. third st.',\n",
        "  4: '6333 w. third st.'},\n",
        " 'city': {0: 'la', 1: 'hollywood', 2: 'pasadena', 3: 'los angeles', 4: 'la'},\n",
        " 'phone': {0: 2139330773,\n",
        "  1: 2138486677,\n",
        "  2: 8187951001,\n",
        "  3: 2136877178,\n",
        "  4: 2139330358},\n",
        " 'cuisine_type': {0: 'american',\n",
        "  1: 'american',\n",
        "  2: 'californian',\n",
        "  3: 'japanese',\n",
        "  4: 'cajun/creole'}}\n",
        "restaurants_new = pd.DataFrame(df_dict)"
      ],
      "metadata": {
        "id": "CSmNq9k52FhT"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an indexer and object and find possible pairs\n",
        "indexer = recordlinkage.Index()\n",
        "\n",
        "# Block pairing on cuisine_type\n",
        "indexer.block('cuisine_type')\n",
        "\n",
        "# Generate pairs\n",
        "pairs = indexer.index(restaurants, restaurants_new)\n",
        "pairs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khZD08ctjkc6",
        "outputId": "01e7e22d-a3e9-4cca-b053-3295d826151c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiIndex([], )"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a comparison object\n",
        "comp_cl = recordlinkage.Compare()\n",
        "\n",
        "# Find exact matches on city, cuisine_types\n",
        "comp_cl.exact('city', 'city', label='city')\n",
        "comp_cl.exact('cuisine_type', 'cuisine_type', label = 'cuisine_type')\n",
        "\n",
        "# Find similar matches of rest_name\n",
        "comp_cl.string('rest_name', 'rest_name', threshold=0.8, label='name')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMrKnMPm2_eE",
        "outputId": "c4b8ee44-727c-4e9a-ec20-626d0a888e22"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Compare>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a comparison object\n",
        "comp_cl = recordlinkage.Compare()\n",
        "\n",
        "# Find exact matches on city, cuisine_types -\n",
        "comp_cl.exact('city', 'city', label='city')\n",
        "comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')\n",
        "\n",
        "# Find similar matches of rest_name\n",
        "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8)\n",
        "\n",
        "# Get potential matches and print\n",
        "potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n",
        "print(potential_matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbxHm-xu3jDh",
        "outputId": "32d139d2-9dfe-4e09-ab4d-8fc7476ea805"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [city, cuisine_type, name]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import recordlinkage\n",
        "# Isolate potential matches with row sum >=3\n",
        "matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n",
        "\n",
        "# Get values of second column index of matches\n",
        "matching_indices = matches.index.get_level_values(1)\n",
        "\n",
        "# Subset restaurants_new based on non-duplicate values\n",
        "non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\n",
        "\n",
        "# Append non_dup to restaurants\n",
        "full_restaurants = restaurants.append(non_dup)\n",
        "print(full_restaurants)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "6HSOQ6eA5bDM",
        "outputId": "961115b5-759e-4316-c18d-b21523b4f472"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'concat'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-ef54093ab638>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Append non_dup to restaurants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfull_restaurants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestaurants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_dup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_restaurants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'concat'"
          ]
        }
      ]
    }
  ]
}